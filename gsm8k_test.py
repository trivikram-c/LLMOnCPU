# -*- coding: utf-8 -*-
"""gsm8k_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EAlzMC4Xl920BRaM9OMeEGhBc9i0GG5b
"""


#!pip install --quiet datasets
#!pip install --quiet -U "huggingface_hub[cli]"
#!pip install --quiet tensorboard
#!pip install --quiet llama-cpp-python

#!huggingface-cli login

import sys
if sys.platform.startswith("linux"):
    import resource

import re
import psutil
from tqdm import tqdm
from datasets import load_dataset
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, StoppingCriteriaList, StoppingCriteria
from torch.profiler import profile, record_function, ProfilerActivity
import time
import json
from llama_cpp import Llama, llama_model_size
import argparse


virtual_memory = psutil.virtual_memory()
available_memory = virtual_memory.available
print(f"Available memory: {available_memory / (1024**3)} GB")
print(f'number of physical cores: {psutil.cpu_count(logical=False)}')
print(f'number of logical cores: {psutil.cpu_count()}')

#memory_constrain = 13
#print(f'limit the memory to {memory_constrain} GB')
#resource.setrlimit(resource.RLIMIT_AS, (memory_constrain * (1024**3), memory_constrain * (1024**3)))

# Using 8-shot Cot as in https://github.com/EleutherAI/lm-evaluation-harness/blob/main/lm_eval/tasks/gsm8k/gsm8k-cot.yamllm-evaluation-harness gsm8k-cot
# Every question in the gsm8k dataset will follow the examplar below:
FEW_SHOT_PROMPT = """ Here are some examples:
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.

Q: If there are 3 cars in the parking lot and 2 more cars arrive, how many cars are in the parking lot?
A: There are originally 3 cars. 2 more cars arrive. 3 + 2 = 5. The answer is 5.

Q: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?
A: Originally, Leah had 32 chocolates. Her sister had 42. So in total they had 32 + 42 = 74. After eating 35, they had 74 - 35 = 39. The answer is 39.

Q: Jason had 20 lollipops. He gave Denny some lollipops. Now Jason has 12 lollipops. How many lollipops did Jason give to Denny?
A: Jason started with 20 lollipops. Then he had 12 after giving some to Denny. So he gave Denny 20 - 12 = 8. The answer is 8.

Q: Shawn has five toys. For Christmas, he got two toys each from his mom and dad. How many toys does he have now?
A: Shawn started with 5 toys. If he got 2 toys each from his mom and dad, then that is 4 more toys. 5 + 4 = 9. The answer is 9.

Q: There were nine computers in the server room. Five more computers were installed each day, from monday to thursday. How many computers are now in the server room?
A: There were originally 9 computers. For each of 4 days, 5 more computers were added. So 5 * 4 = 20 computers were added. 9 + 20 is 29. The answer is 29.

Q: Michael had 58 golf balls. On tuesday, he lost 23 golf balls. On wednesday, he lost 2 more. How many golf balls did he have at the end of wednesday?
A: Michael started with 58 golf balls. After losing 23 on tuesday, he had 58 - 23 = 35. After losing 2 more, he had 35 - 2 = 33 golf balls. The answer is 33.

Q: Olivia has $23. She bought five bagels for $3 each. How much money does she have left?
A: Olivia had 23 dollars. 5 bagels for 3 dollars each will be 5 x 3 = 15 dollars. So she has 23 - 15 dollars left. 23 - 15 is 8. The answer is 8.

Now please answer the following question.
Q: {question}
A:"""

ONE_SHOT_PROMPT = """ Here are some examples:
Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?
A: There are 15 trees originally. Then there were 21 trees after some more were planted. So there must have been 21 - 15 = 6. The answer is 6.

Now please answer the following question.
Q: {question}
A:"""




ZERO_SHOT_COT_PROMPT = """
Q: {question} Let's think step by step.
"""


# load the llama 3.2 1B model and quantize to 8 bits
def load_model_and_tokenizer(model_name="meta-llama/Llama-3.2-1B-Instruct",quantization_scheme=None,device='cpu'):
  assert isinstance(device,str) and device in ['cpu','cuda'], 'not a valid device'
  assert quantization_scheme in [None,8], 'not a valid quantization scheme'

  quant_model_repo_dic = {'meta-llama/Llama-3.2-1B-Instruct':["bartowski/Llama-3.2-1B-Instruct-GGUF","Llama-3.2-1B-Instruct-Q8_0.gguf"],
               'meta-llama/Llama-3.2-3B-Instruct':["bartowski/Llama-3.2-3B-Instruct-GGUF","Llama-3.2-3B-Instruct-Q8_0.gguf"],
               'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B':["bartowski/DeepSeek-R1-Distill-Qwen-1.5B-GGUF","DeepSeek-R1-Distill-Qwen-1.5B-Q8_0.gguf"]}

  if quantization_scheme is None:
    print('Loading model and tokenizer...')

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name) # transformer.AutoModelForCausalLM

    model.to(device)
    return model,tokenizer
  elif quantization_scheme==8:
    print('Loading model in GGUF format')
    llm = Llama.from_pretrained(
	    repo_id = quant_model_repo_dic[model_name][0],
	    filename = quant_model_repo_dic[model_name][1],
      verbose = False,
      n_gpu_layers = 0 if device=='cpu' else -1 # If -1, all layers are offloaded to GPU.

    )
    #llm._n_ctx = 1024
    tokenizer = AutoTokenizer.from_pretrained(model_name) # not used
    return llm,tokenizer
  else:
    print('not implemented yet')

def extract_answer(output):
  '''
  extract the answer from the decoded output of the model
  first get the text after the final A:, then extract the numeric answer from this
  reference: https://github.com/tianlwang/eval_gsm8k/blob/main/utils.py
  '''
  decoded_output = output.split('A:')[-1].strip()
  regex_pattern = "(-?[$0-9.,]{2,})|(-?[0-9]+)"
  '''
  (-?[$0-9.,]{2,})

    -?: This matches an optional negative sign (the - at the start of a number).

    [$0-9.,]{2,}: This matches strings that are at least 2 characters long and may consist of:

      The dollar sign ($)

      Digits (0-9)

      Commas (,)

      Periods (.)

    Together, this part of the regex matches numeric-like values that may include currency symbols,
    commas, and decimal points, e.g., $12.34, -1,234, or 300.5.

  (-?[0-9]+)

    -?: Again, this matches an optional negative sign.

    [0-9]+: This matches one or more digits.

    Together, this part of the regex matches plain integers (positive or negative), e.g., 42 or -99.
  '''
  regexes_to_ignore =[
        ",",
        "\\$",
        "(?s).*#### ",
        "\\.$"
    ]

  answer = re.findall(regex_pattern, decoded_output) # a list of tuples
  if answer:
    answer = answer[-1]
    if isinstance(answer,tuple):
      answer = [a for a in answer if a]
      answer = answer[0].strip() # get the first element
      for regex in regexes_to_ignore:
        answer = re.sub(regex, "", answer) # remove the patterns in regexes_to_ignore
      return answer
  else:
    return None

# define stopping criteria
class KeywordsStoppingCriteria(StoppingCriteria):
    def __init__(self, keywords_list, tokenizer, start_len):
        #self.keywords_list = keywords_list # a list of strings
        self.tokenizer = tokenizer
        self.start_len = start_len  # Initialize start_len
        self.keywords_list = keywords_list # stop strings
        #self.last_character_deq = deque(maxlen=3)
        #self.countdown = 10
        #self.flag = False

    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:

        current_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)[self.start_len:] # start from the last answer

        '''
        last_character = self.tokenizer.decode(input_ids[0][-1],skip_special_tokens=True).strip() # decoded from the last token
        self.last_character_deq.append(last_character)

        #print(last_character,self.flag,self.countdown)

        # flag == True when the last 3 words are ['is','answer','The']
        if not self.flag and len(self.last_character_deq) == 3 and self.last_character_deq[-1] == 'is' and \
          self.last_character_deq[-2] == 'answer' and self.last_character_deq[-3] == 'The':
          self.flag = True
        if self.flag: # if flag is True, then only generate 10 more tokens
          self.countdown -= 1
        if self.countdown == 0:
          return True
        '''
        return any(keyword in current_text for keyword in self.keywords_list)

def memory_profiling(model,tokenizer,keywords_list,prompt,device='cpu',latency_measure=1,quantization=False):
  '''
  send the prompt to the model and record the memory footprint

  parameters:
  model: the model
  tokenizer: the tokenizer
  keywords_list: a list of strings, indicating the stopping criteria
  prompt: the formatted prompt
  device: the device to run the model on
  latency_measure: an integer, 0 means generating only one output token, 1 means generating maximum 512 new tokens
  quantization: a boolean, whether to the model is a GGUF file
  '''
  assert isinstance(device,str) and device in ['cpu','cuda'], 'not a valid device'
  if device=='cuda':
    print(f"Inference is on GPU: {torch.cuda.get_device_name(0)}")
    return

  # define stop criteria list
  stopping_criteria = KeywordsStoppingCriteria(keywords_list,tokenizer,len(prompt)) # define stopping criteria
  stopping_criteria_list = StoppingCriteriaList([stopping_criteria])
  tokenized_prompt = tokenizer(prompt,return_tensors='pt').to(device) # tokenize the prompt

  max_new_tokens = 256 if latency_measure==1 else 1
  print('in memory profiling')
  with profile(activities=[ProfilerActivity.CPU], profile_memory=True, on_trace_ready=torch.profiler.tensorboard_trace_handler('./log')) as prof:
    if not quantization:
      model.generate(**tokenized_prompt,max_new_tokens=max_new_tokens,stopping_criteria=stopping_criteria_list)
    else:

      print('memory profiling is not implemented in llama_cpp')
      '''
      output = model(
            prompt, # Prompt
            max_tokens=max_new_tokens, # Generate up to 32 tokens, set to None to generate up to the end of the context window
            stop=keywords_list, # Stop generating just before the model would generate a new question
            echo=False # Echo the prompt back in the output
      ) # Generate a completion, can also call create_completion
      '''
  #print(prof.key_averages().table(sort_by="self_cpu_memory_usage", row_limit=10))

def write_result(model_name,accuracy,total_output_token_count,total_generation_time_l,TTFT_l,latency_measure,footprint):
  '''
  please use this function to write the result to a json file after each experiment
  parameters:
  model_name: the name of the model
  accuracy: a float, the accuracy of the model on the dataset
  total_output_token_count: a list, where each item is the total output token of one response
  total_generation_time_l: a list, where each item is the total generation time of one response
  latency_measure: 0 or 1. 0 means to measure the TTFT of the model, 1 means to measure the total generation time of the model
  footprint: a float, the memory footprint of the model in GB
  '''
  s = 'TTFT' if latency_measure==0 else 'Total_generation_time'
  model_name = model_name.split('/')[-1]
  dic = {'model loading footprint (GB)':footprint,'accuracy':accuracy,'total_output_token_count':total_output_token_count,'total_generation_time_l':total_generation_time_l,'TTFT_l':TTFT_l}
  with open(f'{model_name}_{s}_result.json','w') as f:
    f.write(json.dumps(dic))
  # use json.load(open('result.json','r')) to get the result back



def evaluate_on_gsm8k(model, tokenizer, template_prompt, dataset_name='gsm8k', limit=1.0, latency_measure=1, device='cpu', verbose=True, do_memory_profiling=False, quantization=False,
                      memory_constrain=-1):
  '''
  parameters:
  model: the model
  tokenizer: the tokenizer
  template_prompt: the prompt template, template = FEW_SHOT_PROMPT means 8-shot Cot, template = ZERO_SHOT_COT_PROMPT means zero-shot Cot
  dataset_name: the name of the dataset
  limit: a float number between 0 and 1, indicating the percentage of the dataset to be used, or an integer that indicates which samples to use
  latency_measure: an integer, 0 means to measure the TTFT of the model, 1 means to measure the total generation time of the model
           for an explanation of TTFT and total generation time, see
           https://symbl.ai/developers/blog/a-guide-to-llm-inference-performance-monitoring/#:~:text=There%20are%20several%20ways%20to%20measure%20a%20model%E2%80%99s,Per%20Output%20Token%20%28TPOT%29%203%20Total%20generation%20time
  device: the device to run the model on
  verbose: a boolean, whether to print the accuracy, total output token count, total generation time, and TTFT
  do_memory_profiling: a boolean, whether to do memory profiling of the model
             the result is valid only when quantization=False and device='cpu'
             the memory footprint is obtained by sending one sample to the model and recording the memory footprint
             during the inference.
             Not implemented when using llama_cpp to do the inference.
  quantization: a boolean, whether the model is a GGUF file
  memory_constrain: the memory constrain applied on the process in GB. If -1, no constrain is applied.

  returns:
  accuracy: a float, the accuracy of the model on the dataset. Discarded if latency_measure=0
  total_output_token: a list, where each item is the total output token of one response. Discarded if latency_measure=0
  total_generation_time_l: a list, where each item is the total generation time of one response. Discarded if latency_measure=0
  TTFT_l: a list, where each item is the TTFT of one response. Discarded if latency_measure=1

  '''


  # evalute the model on gsm8k dataset
  assert isinstance(device,str) and device in ['cpu','cuda'], 'not a valid device'
  assert latency_measure in [0,1], 'latency_measure must be 0 or 1'
  assert isinstance(limit,(int,float)), 'not a valid limit'
  assert isinstance(verbose,bool), 'not a valid verbose'
  assert isinstance(do_memory_profiling,bool), 'not a valid memory_profiling'
  assert isinstance(quantization,bool), 'not a valid quantization'
  assert isinstance(memory_constrain,int) and memory_constrain>=-1, 'not a valid memory_constrain'

  # limit the memory
  if memory_constrain!=-1 and sys.platform.startswith("linux"):
    print(f'limit the memory to {memory_constrain} GB')
    resource.setrlimit(resource.RLIMIT_AS, (memory_constrain * (1024**3), memory_constrain * (1024**3)))

  # define stop criteria list
  keywords_list = ['Q:','Wait',"</s>","<|im_end|>"]


  # load dataset
  print(f'loading {dataset_name} dataset')
  dataset = load_dataset(dataset_name, "main", split='test')
  dataset_len = len(dataset)
  if isinstance(limit,int):
    assert 0 < limit <= dataset_len, 'limit must be less than or equal to the length of the dataset and greater than 0'
    dataset_len = limit
  else:
    assert 0 < limit <= 1, 'limit must be greater than 0 and less than or equal to 1'
    dataset_len = int(limit * dataset_len)
  #dataset = dataset[:dataset_len].to(device)

  # peek the first item of the dataset
  if verbose:
    print('peek the first item of the dataset')
    print(dataset[0])
    print('\n')


  correct_count = 0
  total_output_token_count = 0
  total_generation_time_l = []
  TTFT_l = []
  #peak_memory_usage_l = []

  #tracemalloc.start()
  for i in tqdm(range(dataset_len)):
    sample = dataset[i]
    #print(sample)

    prompt = template_prompt.format(question=sample['question'])# concatenate the template prompt to the question

    if do_memory_profiling and i==0:
      memory_profiling(model,tokenizer,keywords_list,prompt,device,latency_measure=latency_measure,quantization=quantization) # only do memory profiling on the first sample


    stopping_criteria = KeywordsStoppingCriteria(keywords_list,tokenizer,len(prompt)) # define stopping criteria
    stopping_criteria_list = StoppingCriteriaList([stopping_criteria])

    answer = sample['answer'].split('####')[-1].strip() # ground truth is the number after the #### of the answer
    #print(answer)
    tokenized_prompt = tokenizer(prompt,return_tensors='pt').to(device) # tokenize the prompt
    input_token_count = len(tokenized_prompt['input_ids'].flatten()) # token number of this iteration


    max_new_tokens = 256 if latency_measure==1 else 1



    t1 = time.perf_counter()
    if not quantization:
      output = model.generate(**tokenized_prompt,max_new_tokens=max_new_tokens,stopping_criteria=stopping_criteria_list) # as per the setting given by meta
    else:
      output = model(
            prompt, # Prompt
            max_tokens=max_new_tokens, # Generate up to 32 tokens, set to None to generate up to the end of the context window
            stop=keywords_list, # Stop generating just before the model would generate a new question
            echo=True # Echo the prompt back in the output
      ) # Generate a completion, can also call create_completion

    t2 = time.perf_counter()



    processing_time = t2-t1 # in s, total generation time of one response if latency_measure==1, TTFT if latency_measure=0

    if not quantization:
      total_output_token_count += len(output.flatten())-input_token_count # total output token count of one response
    else:
      total_output_token_count += output['usage']["completion_tokens"]

    total_generation_time_l.append(processing_time)
    TTFT_l.append(processing_time)

    if not quantization:
      decoded_output = tokenizer.decode(output[0], skip_special_tokens=True) # decode the output
    else:
      decoded_output = output['choices'][0]["text"]

    #print(decoded_output)
    if verbose and i<5:
      print('the first decoded output is:')
      print(decoded_output) # just for checking

      #pass


    decoded_output = extract_answer(decoded_output) # extract the numerical answer from the decoded output
    if verbose and i<5:
      print('extracted answer:',decoded_output)
      print('answer:',answer)
    if decoded_output == answer:
      correct_count += 1
    #break


  accuracy = correct_count/dataset_len # accuracy of the model on the dataset
  if latency_measure==1:
    print('\n')
    print(f'Accuracy: {accuracy}')
    print(f'Total output token count: {total_output_token_count}')
    print(f'Total processing time: {sum(total_generation_time_l)} s')

  else:
    print('\nplease disregard the accuracy, total_output_token_count, total_generation_time_l in the output')
  if do_memory_profiling:
    print('check the log file to see memory footprint')

  return accuracy,total_output_token_count,total_generation_time_l,TTFT_l




def main():
  parser = argparse.ArgumentParser(description='evaluate on gsm8k')
  parser.add_argument('--model', type=int, default=0, help='select the model. 0 = Llama-3.2-1B-Instruct, 1 = Llama-3.2-3B-Instruct, 2 = DeepSeek-R1-Distill-Qwen-1.5B')
  parser.add_argument('--device', type=str, default='cpu', help='select the device (cpu or gpu)')
  parser.add_argument('--quantization_scheme', type=int, default=-1, help='select the quantization scheme. -1 = no quantization, 8 = Q8_0 quantization')
  parser.add_argument('--prompt', type=int, default=1, help='select the quantization scheme. 0 = zero-shot Cot, 1 = one-shot Cot, 8 = 8-shot Cot')
  parser.add_argument('--limit', default=1.0, help='a float number between 0 and 1, indicating the percentage of the dataset to be used, or an integer that indicates how many samples to use')
  parser.add_argument('--latency_measure', type=int, default=1, help='an integer, 0 means to measure the TTFT of the model, 1 means to measure the total generation time of the model')
  parser.add_argument('--verbose', type=bool, default=True, help='a boolean, whether to print the accuracy, total output token count, total generation time, and TTFT')
  parser.add_argument('--do_memory_profiling', type=bool, default=False, help='''a boolean, whether to do memory profiling of the model
                                                                                 the result is valid only when quantization=False and device=cpu
                                                                                 the memory footprint is obtained by sending one sample to the model and recording the memory footprint
                                                                                 during the inference.
                                                                                 Not implemented when using llama_cpp to do the inference.''')
  parser.add_argument('--memory_constrain', type=int, default=-1, help='the memory constrain applied on the process in GB. If -1, no constrain is applied. Only valid in linux platform.')



  args = parser.parse_args()
  assert isinstance(args.model,int) and args.model in [0,1,2],'not a valid model'
  assert isinstance(args.quantization_scheme,int) and args.quantization_scheme in [-1,8],'not a valid quantization scheme.'
  assert isinstance(args.prompt, int) and args.prompt in [0,1,8], 'not a valid prompt.'
  assert isinstance(args.limit, (int, float)), 'not a valid limit'
  assert args.latency_measure in [0, 1], 'latency_measure must be 0 or 1'
  assert isinstance(args.verbose, bool), 'not a valid verbose'
  assert isinstance(args.do_memory_profiling, bool), 'not a valid memory_profiling'
  assert isinstance(args.memory_constrain, int) and args.memory_constrain >= -1, 'not a valid memory_constrain'


  if args.device=='gpu' and torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)} is available. Run inference on GPU.")
    device = 'cuda'
  else:
    print("Run inference on CPU.")
    device = 'cpu'

  quantization_scheme = None if args.quantization_scheme==-1 else 8
  model_name_l = ['meta-llama/Llama-3.2-1B-Instruct','meta-llama/Llama-3.2-3B-Instruct',
           'deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B']

  if args.prompt==0:
    prompt = ZERO_SHOT_COT_PROMPT
  elif args.prompt==1:
    prompt = ONE_SHOT_PROMPT
  else:
    prompt = FEW_SHOT_PROMPT

  # get the model and tokenizer

  model,tokenizer = load_model_and_tokenizer(model_name=model_name_l[args.model],quantization_scheme=quantization_scheme,device=device)



  if quantization_scheme is None:
    footprint = model.get_memory_footprint()/(1024**3)
    print(f'model loading memory footprint: {footprint} GB')
  else:
    footprint = llama_model_size(model.model)/(1024**3)
    if device=='cpu':
      print(f'run quantized model on cpu.')
      print(f'number of threads used: {model.n_threads}')
      print(f'max number of physical cores: {psutil.cpu_count()}')
    print(f'model loading memory footprint: {footprint} GB')

  accuracy,total_output_token_count,total_generation_time_l,TTFT_l = evaluate_on_gsm8k(model,tokenizer,prompt,limit=args.limit,latency_measure=args.latency_measure,
                                                                                       device=device,verbose=args.verbose,do_memory_profiling=args.do_memory_profiling,quantization=quantization_scheme != None,
                                                                                       memory_constrain=args.memory_constrain)

  write_result(model_name_l[args.model],accuracy,total_output_token_count,total_generation_time_l,TTFT_l,latency_measure=0,footprint=footprint)
  #


  print(f'model name: {model_name_l[args.model]}')
  print(f'Accuracy: {accuracy}')
  print(f'Total output token count: {total_output_token_count}')
  print(f'average total generation time: {sum(total_generation_time_l)/len(total_generation_time_l)} s')
  print(f'average TTFT:{sum(TTFT_l)/len(TTFT_l)} s')
  print(f'throughput: {total_output_token_count/sum(total_generation_time_l)} token/s')

# do not run this cell
# cannot visit the local server on Colab. Need to download the log file and see it locally.
# to see the log, create a virtual environment in your machine, run the command:
# pip install torch_tb_profiler
# After installing, run
# tensorboard --logdir=./log


if __name__ == '__main__':
  main()